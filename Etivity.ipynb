{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Student Name:\n",
    "#Student ID:\n",
    "\n",
    "#Student Name:\n",
    "#Student ID:\n",
    "\n",
    "#Student Name:\n",
    "#Student ID:\n",
    "\n",
    "#Student Name:\n",
    "#Student ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following code is from https://github.com/amclean3695/Communities-and-Crime/blob/master/Communities_and_Crime.ipynb\n",
    "# TODO : See is there any other means for pulling column names from C45 file format\n",
    "#Reads in names from textfile and performs regex to extract the attribute names \n",
    "textfile = open('communities.names','r')\n",
    "filetext = textfile.read()\n",
    "textfile.close()\n",
    "matches = re.findall(r'-{2}\\s(\\w+)\\:{1}', filetext)\n",
    "col_names = matches[2:] #corresponds to the 128 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 128)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('communities.data', header=None, names=col_names)\n",
    "\n",
    "#data_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data'\n",
    "#df = pd.read_csv(data_url, header=None)\n",
    "# The above code does not work as PythonAnywhere blocks outgoing connections for free accounts to non-whitelisted sites\n",
    "\n",
    "#print(df.head(5))\n",
    "#dir(df)\n",
    "print(df.shape)\n",
    "#print(df['ViolentCrimesPerPop'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-Predictive Attributes\n",
    "\n",
    "Of the 128 attributes loaded, for this dataset it is stated that:\n",
    "\n",
    "Attribute Information: (122 predictive, 5 non-predictive, 1 goal) \n",
    "* state: US state (by number) - not counted as predictive above, but if considered, should be consided nominal (nominal) \n",
    "* county: numeric code for county - not predictive, and many missing values (numeric) \n",
    "* community: numeric code for community - not predictive and many missing values (numeric) \n",
    "* communityname: community name - not predictive - for information only (string) \n",
    "* fold: fold number for non-random 10 fold cross validation, potentially useful for debugging, paired tests - not predictive (numeric) \n",
    "\n",
    "Therefore, the first five columns shall be removed from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 123)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(['state', 'county', 'community', 'communityname', 'fold'], axis=1)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViolentCrimesPerPop    0\n",
       "PctNotHSGrad           0\n",
       "PctUnemployed          0\n",
       "PctEmploy              0\n",
       "PctEmplManu            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df.head(5))\n",
    "# From DePy_Talk Noteboook\n",
    "# How much of your data is missing?\n",
    "df.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no cells with missing data\n",
    "However, visually inspecting the data, there are cells with \"?\" and this needs to be replaced with \"NaN\" (Not a Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['OtherPerCap', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n",
      "       'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop',\n",
      "       'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol',\n",
      "       'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp', 'PctPolicAsian',\n",
      "       'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz',\n",
      "       'PolicAveOTWorked', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr',\n",
      "       'LemasGangUnitDeploy', 'PolicBudgPerPop'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "print(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LemasSwFTFieldPerPop    1675\n",
       "PctPolicAsian           1675\n",
       "PolicBudgPerPop         1675\n",
       "LemasSwornFT            1675\n",
       "LemasSwFTPerPop         1675\n",
       "LemasSwFTFieldOps       1675\n",
       "LemasTotalReq           1675\n",
       "LemasTotReqPerPop       1675\n",
       "PolicReqPerOffic        1675\n",
       "PolicPerPop             1675\n",
       "RacialMatchCommPol      1675\n",
       "PctPolicBlack           1675\n",
       "PctPolicHisp            1675\n",
       "PctPolicWhite           1675\n",
       "PctPolicMinor           1675\n",
       "PolicCars               1675\n",
       "LemasGangUnitDeploy     1675\n",
       "LemasPctPolicOnPatr     1675\n",
       "OfficAssgnDrugUnits     1675\n",
       "PolicOperBudg           1675\n",
       "PolicAveOTWorked        1675\n",
       "NumKindsDrugsSeiz       1675\n",
       "OtherPerCap                1\n",
       "PctLess9thGrade            0\n",
       "PctPopUnderPov             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 123)\n",
      "1994\n",
      "1675\n",
      "Column  LemasSwornFT 1675\n",
      "Column  LemasSwFTPerPop 1675\n",
      "Column  LemasSwFTFieldOps 1675\n",
      "Column  LemasSwFTFieldPerPop 1675\n",
      "Column  LemasTotalReq 1675\n",
      "Column  LemasTotReqPerPop 1675\n",
      "Column  PolicReqPerOffic 1675\n",
      "Column  PolicPerPop 1675\n",
      "Column  RacialMatchCommPol 1675\n",
      "Column  PctPolicWhite 1675\n",
      "Column  PctPolicBlack 1675\n",
      "Column  PctPolicHisp 1675\n",
      "Column  PctPolicAsian 1675\n",
      "Column  PctPolicMinor 1675\n",
      "Column  OfficAssgnDrugUnits 1675\n",
      "Column  NumKindsDrugsSeiz 1675\n",
      "Column  PolicAveOTWorked 1675\n",
      "Column  PolicCars 1675\n",
      "Column  PolicOperBudg 1675\n",
      "Column  LemasPctPolicOnPatr 1675\n",
      "Column  LemasGangUnitDeploy 1675\n",
      "Column  PolicBudgPerPop 1675\n",
      "(1994, 101)\n"
     ]
    }
   ],
   "source": [
    "#print(df.isnull().sum())\n",
    "print(df.shape)\n",
    "print(df.shape[0])\n",
    "print(df['LemasSwFTFieldPerPop'].isnull().sum())\n",
    "for column in df:\n",
    "    if(df[column].isnull().sum()>=(df.shape[0]/2)):\n",
    "        print(\"Column \", column, df[column].isnull().sum())\n",
    "        df = df.drop([column], axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OtherPerCap            1\n",
       "PctOccupManu           0\n",
       "HispPerCap             0\n",
       "NumUnderPov            0\n",
       "PctPopUnderPov         0\n",
       "PctLess9thGrade        0\n",
       "PctNotHSGrad           0\n",
       "PctBSorMore            0\n",
       "PctUnemployed          0\n",
       "PctEmploy              0\n",
       "PctEmplManu            0\n",
       "PctEmplProfServ        0\n",
       "ViolentCrimesPerPop    0\n",
       "NumIlleg               0\n",
       "MalePctDivorce         0\n",
       "MalePctNevMarr         0\n",
       "FemalePctDiv           0\n",
       "TotalPctDiv            0\n",
       "PersPerFam             0\n",
       "PctFam2Par             0\n",
       "PctKids2Par            0\n",
       "PctYoungKids2Par       0\n",
       "PctTeen2Par            0\n",
       "PctWorkMomYoungKids    0\n",
       "PctOccupMgmtProf       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one missing value remaining but lets impute it for the craic using code from the DePy_Talk notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/kernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# print(df['OtherPerCap'][130])\n",
    "\n",
    "# # dir(sklearn)\n",
    "# # from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# # imp.fit(df) \n",
    "# # X = pd.DataFrame(data=imp.transform(df) , columns=df.columns)\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# my_imputer = SimpleImputer()\n",
    "\n",
    "#TEMP WORKAROUND\n",
    "# Set to median\n",
    "print(df['OtherPerCap'][130])\n",
    "#print(df['OtherPerCap'].median())\n",
    "#print(df['OtherPerCap'].mean())\n",
    "dir(df['OtherPerCap'])\n",
    "miss_median = df['OtherPerCap'].median()\n",
    "df['OtherPerCap'][130] = miss_median\n",
    "print(df['OtherPerCap'][130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From DePy_Talk Notebook\n",
    "# def find_outliers_tukey(x):\n",
    "#     q1 = np.percentile(x, 25)\n",
    "#     q3 = np.percentile(x, 75)\n",
    "#     iqr = q3-q1\n",
    "#     floor = q1 - 1.5*iqr\n",
    "#     ceiling = q3 + 1.5*iqr\n",
    "#     outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n",
    "#     outlier_values = list(x[outlier_indices])\n",
    "    \n",
    "#     return outlier_indices, outlier_values\n",
    "\n",
    "# tukey_indices, tukey_values = find_outliers_tukey(df['householdsize'])\n",
    "\n",
    "# print(np.sort(tukey_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split Up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 101)\n",
      "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
      "0        0.19           0.33          0.02          0.90          0.12   \n",
      "1        0.00           0.16          0.12          0.74          0.45   \n",
      "2        0.00           0.42          0.49          0.56          0.17   \n",
      "3        0.04           0.77          1.00          0.08          0.12   \n",
      "4        0.01           0.55          0.02          0.95          0.09   \n",
      "\n",
      "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
      "0         0.17         0.34         0.47         0.29        0.32   \n",
      "1         0.07         0.26         0.59         0.35        0.27   \n",
      "2         0.04         0.39         0.47         0.28        0.32   \n",
      "3         0.10         0.51         0.50         0.34        0.21   \n",
      "4         0.05         0.38         0.38         0.23        0.36   \n",
      "\n",
      "          ...           PctForeignBorn  PctBornSameState  PctSameHouse85  \\\n",
      "0         ...                     0.12              0.42            0.50   \n",
      "1         ...                     0.21              0.50            0.34   \n",
      "2         ...                     0.14              0.49            0.54   \n",
      "3         ...                     0.19              0.30            0.73   \n",
      "4         ...                     0.11              0.72            0.64   \n",
      "\n",
      "   PctSameCity85  PctSameState85  LandArea  PopDens  PctUsePubTrans  \\\n",
      "0           0.51            0.64      0.12     0.26            0.20   \n",
      "1           0.60            0.52      0.02     0.12            0.45   \n",
      "2           0.67            0.56      0.01     0.21            0.02   \n",
      "3           0.64            0.65      0.02     0.39            0.28   \n",
      "4           0.61            0.53      0.04     0.09            0.02   \n",
      "\n",
      "   LemasPctOfficDrugUn  ViolentCrimesPerPop  \n",
      "0                 0.32                 0.20  \n",
      "1                 0.00                 0.67  \n",
      "2                 0.00                 0.43  \n",
      "3                 0.00                 0.12  \n",
      "4                 0.00                 0.03  \n",
      "\n",
      "[5 rows x 101 columns]\n",
      "(1994, 101)\n",
      "[0.19 0.33 0.02 0.9 0.12 0.17 0.34 0.47 0.29 0.32 0.2 1.0 0.37 0.72 0.34\n",
      " 0.6 0.29 0.15 0.43 0.39 0.4 0.39 0.32 0.27 0.27 '0.36' 0.41 0.08 0.19 0.1\n",
      " 0.18 0.48 0.27 0.68 0.23 0.41 0.25 0.52 0.68 0.4 0.75 0.75 0.35 0.55 0.59\n",
      " 0.61 0.56 0.74 0.76 0.04 0.14 0.03 0.24 0.27 0.37 0.39 0.07 0.07 0.08\n",
      " 0.08 0.89 0.06 0.14 0.13 0.33 0.39 0.28 0.55 0.09 0.51 0.5 0.21 0.71 0.52\n",
      " 0.05 0.26 0.65 0.14 0.06 0.22 0.19 0.18 0.36 0.35 0.38 0.34 0.38 0.46\n",
      " 0.25 0.04 0.0 0.12 0.42 0.5 0.51 0.64 0.12 0.26 0.2 0.32]\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head())\n",
    "dataset = df.values\n",
    "print(dataset.shape)\n",
    "X = dataset[:, 0:100]\n",
    "y = dataset[:, 100]\n",
    "print(X[0])\n",
    "#X = df[:,0:100].values\n",
    "#y = df[:,100].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ul9939245/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ul9939245/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ul9939245/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Scale the dataset\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# Scale the data with respect to the training data set \n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "\n",
    "X_scaled_train = scaler.transform(X_train) \n",
    "# Scaling the test set using the transform defined by the train set. \n",
    "X_scaled_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1595, 100)\n",
      "(399, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled_train.shape)\n",
    "print(X_scaled_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.          0.         -0.00884085  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.         -0.         -0.         -0.          0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.          0.          0.          0.\n",
      "  0.         -0.          0.         -0.         -0.         -0.\n",
      "  0.         -0.          0.          0.          0.          0.\n",
      "  0.         -0.         -0.04255819 -0.         -0.         -0.\n",
      " -0.          0.          0.02973777  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      "  0.         -0.          0.          0.         -0.          0.\n",
      " -0.         -0.          0.         -0.         -0.          0.\n",
      "  0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.          0.          0.         -0.          0.\n",
      "  0.          0.         -0.         -0.          0.         -0.\n",
      "  0.          0.          0.          0.        ]\n",
      "0.24080877742946682\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = clf.predict(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399,)\n",
      "(399,)\n",
      "[0.14694148 0.2124953  0.21826913 0.14784458 0.24289814]\n",
      "[0.06 0.37 0.22 0.01 0.16]\n",
      "0.38237857081575977\n"
     ]
    }
   ],
   "source": [
    "print(y_hat.shape)\n",
    "print(y_test.shape)\n",
    "print(y_hat[0:5])\n",
    "print(y_test[0:5])\n",
    "print(clf.score(X_scaled_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14694148 0.2124953  0.21826913 0.14784458 0.24289814 0.23580896\n",
      " 0.25402694 0.2123818  0.28479683 0.29305254 0.28115104 0.1389661\n",
      " 0.20613306 0.21422139 0.15303933 0.18130534 0.19264976 0.32337114\n",
      " 0.15724881 0.30080873]\n",
      "[0.06 0.37 0.22 0.01 0.16 0.08 0.1 0.1 0.28 0.28 0.46 0.03 0.09 0.22 0.05\n",
      " 0.18 0.02 0.28 0.02 0.4]\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(y_test, y_hat)\n",
    "print(y_hat[0:20])\n",
    "print(y_test[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE  0.13532431502466122\n",
      "MSE  0.0316077698893399\n",
      "R2S  0.38237857081575977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"MAE \", mean_absolute_error(y_test, y_hat))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE \", mean_squared_error(y_test, y_hat))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2S \", r2_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002654833812690498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso_cv = LassoCV(alphas = None, cv = 10, max_iter = 100000, random_state=1) \n",
    "lasso_cv.fit(X_scaled_train, y_train)\n",
    "print(lasso_cv.alpha_)\n",
    "#print(lasso_cv.mse_path_)\n",
    "#print(lasso_cv.alphas_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01685027 0.22750934 0.22395349 0.06336561 0.26919941 0.06867553\n",
      " 0.25835258 0.19704881 0.3652667  0.37689063 0.41573788 0.03772242\n",
      " 0.14405546 0.28489078 0.05806651 0.14266713 0.09914984 0.41202055\n",
      " 0.03072306 0.31259567]\n",
      "[0.06 0.37 0.22 0.01 0.16 0.08 0.1 0.1 0.28 0.28 0.46 0.03 0.09 0.22 0.05\n",
      " 0.18 0.02 0.28 0.02 0.4]\n",
      "MAE  0.09440412324039879\n",
      "MSE  0.01991586635506697\n",
      "R2S  0.6108404394007163\n"
     ]
    }
   ],
   "source": [
    "y_hat_cv = lasso_cv.predict(X_scaled_test)\n",
    "print(y_hat_cv[0:20])\n",
    "print(y_test[0:20])\n",
    "print(\"MAE \", mean_absolute_error(y_test, y_hat_cv))\n",
    "print(\"MSE \", mean_squared_error(y_test, y_hat_cv))\n",
    "print(\"R2S \", r2_score(y_test, y_hat_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.52014345e+01 1.70928393e+01 9.24995793e+00 7.51310214e+00\n",
      " 5.65483196e+00 4.18733429e+00 3.12493518e+00 2.93720440e+00\n",
      " 2.04961291e+00 1.57447535e+00 1.48947512e+00 1.44793121e+00\n",
      " 1.31392896e+00 1.05018140e+00 9.48935640e-01 8.93173588e-01\n",
      " 8.59924189e-01 7.56221570e-01 7.00031745e-01 6.42938864e-01\n",
      " 6.40089418e-01 5.89827402e-01 5.42676318e-01 5.26226642e-01\n",
      " 4.90120834e-01 4.69805680e-01 4.58751775e-01 4.43615837e-01\n",
      " 4.02068254e-01 3.90904462e-01 3.67367516e-01 3.51412481e-01\n",
      " 3.28485534e-01 3.20834777e-01 2.93141963e-01 2.83276566e-01\n",
      " 2.59503733e-01 2.58189057e-01 2.45232846e-01 2.25216612e-01\n",
      " 2.12488957e-01 2.06977246e-01 1.97897008e-01 1.89709009e-01\n",
      " 1.77319847e-01 1.70157385e-01 1.61053608e-01 1.41179674e-01\n",
      " 1.37300868e-01 1.29464590e-01 1.12306670e-01 1.06415920e-01\n",
      " 1.05196543e-01 1.03554550e-01 8.50974162e-02 8.17662908e-02\n",
      " 7.64009192e-02 7.35851516e-02 6.81518439e-02 6.55112705e-02\n",
      " 6.12743951e-02 6.02053821e-02 5.67368650e-02 5.07482317e-02\n",
      " 4.80707947e-02 4.55470052e-02 4.32603535e-02 4.09628944e-02\n",
      " 3.90372826e-02 3.63849249e-02 3.54425369e-02 3.36627883e-02\n",
      " 3.03690529e-02 2.95116075e-02 2.76709926e-02 2.43469156e-02\n",
      " 2.31872116e-02 2.22425131e-02 2.02398777e-02 1.90280373e-02\n",
      " 1.82087162e-02 1.72356587e-02 1.48682262e-02 1.40047726e-02\n",
      " 1.18047326e-02 1.05106629e-02 9.01549022e-03 7.82382074e-03\n",
      " 6.15558856e-03 5.44644278e-03 4.85080493e-03 4.18342664e-03\n",
      " 3.70302136e-03 2.53400782e-03 2.10875196e-03 1.69184266e-03\n",
      " 1.35300294e-03 1.10170611e-03 7.91457314e-04 6.60732228e-04]\n",
      "100\n",
      "(1595, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "100.06273525721464\n",
      "[2.51856342e-01 1.70821228e-01 9.24415859e-02 7.50839173e-02\n",
      " 5.65128661e-02 4.18470900e-02 3.12297597e-02 2.93536289e-02\n",
      " 2.04832788e-02 1.57348822e-02 1.48854128e-02 1.44702342e-02\n",
      " 1.31310518e-02 1.04952297e-02 9.48340696e-03 8.92613604e-03\n",
      " 8.59385052e-03 7.55747450e-03 6.99592853e-03 6.42535767e-03\n",
      " 6.39688108e-03 5.89457604e-03 5.42336082e-03 5.25896720e-03\n",
      " 4.89813548e-03 4.69511131e-03 4.58464156e-03 4.43337708e-03\n",
      " 4.01816173e-03 3.90659381e-03 3.67137192e-03 3.51192159e-03\n",
      " 3.28279587e-03 3.20633627e-03 2.92958175e-03 2.83098963e-03\n",
      " 2.59341035e-03 2.58027183e-03 2.45079095e-03 2.25075411e-03\n",
      " 2.12355735e-03 2.06847480e-03 1.97772934e-03 1.89590070e-03\n",
      " 1.77208674e-03 1.70050703e-03 1.60952634e-03 1.41091160e-03\n",
      " 1.37214786e-03 1.29383421e-03 1.12236258e-03 1.06349201e-03\n",
      " 1.05130590e-03 1.03489626e-03 8.50440636e-04 8.17150266e-04\n",
      " 7.63530189e-04 7.35390167e-04 6.81091155e-04 6.54701976e-04\n",
      " 6.12359785e-04 6.01676358e-04 5.67012933e-04 5.07164146e-04\n",
      " 4.80406563e-04 4.55184491e-04 4.32332310e-04 4.09372123e-04\n",
      " 3.90128078e-04 3.63621130e-04 3.54203158e-04 3.36416831e-04\n",
      " 3.03500127e-04 2.94931049e-04 2.76536440e-04 2.43316511e-04\n",
      " 2.31726742e-04 2.22285680e-04 2.02271881e-04 1.90161075e-04\n",
      " 1.81973001e-04 1.72248527e-04 1.48589044e-04 1.39959922e-04\n",
      " 1.17973316e-04 1.05040731e-04 9.00983787e-05 7.81891552e-05\n",
      " 6.15172925e-05 5.44302808e-05 4.84776367e-05 4.18080381e-05\n",
      " 3.70069972e-05 2.53241910e-05 2.10742986e-05 1.69078194e-05\n",
      " 1.35215467e-05 1.10101539e-05 7.90961102e-06 6.60317975e-06]\n",
      "Number of PCA components that contribute >= 0.05 % of variance is  64\n",
      "[25.20143447 17.09283934  9.24995793  7.51310214  5.65483196  4.18733429\n",
      "  3.12493518  2.9372044   2.04961291  1.57447535  1.48947512  1.44793121\n",
      "  1.31392896  1.0501814   0.94893564  0.89317359  0.85992419  0.75622157\n",
      "  0.70003174  0.64293886  0.64008942  0.5898274   0.54267632  0.52622664\n",
      "  0.49012083  0.46980568  0.45875178  0.44361584  0.40206825  0.39090446\n",
      "  0.36736752  0.35141248  0.32848553  0.32083478  0.29314196  0.28327657\n",
      "  0.25950373  0.25818906  0.24523285  0.22521661  0.21248895  0.20697725\n",
      "  0.197897    0.189709    0.17731984  0.17015737  0.1610536   0.14117964\n",
      "  0.13730084  0.12946457  0.1123064   0.10641503  0.10519588  0.10355381\n",
      "  0.08509279  0.08176424  0.07639653  0.07355501  0.06807952  0.065476\n",
      "  0.06124905  0.06018854  0.05668888  0.05064638]\n",
      "64\n",
      "(1595, 64)\n",
      "(399, 64)\n",
      "<class 'numpy.ndarray'>\n",
      "99.40537408544589\n",
      "[0.25185634 0.17082123 0.09244159 0.07508392 0.05651287 0.04184709\n",
      " 0.03122976 0.02935363 0.02048328 0.01573488 0.01488541 0.01447023\n",
      " 0.01313105 0.01049523 0.00948341 0.00892614 0.00859385 0.00755747\n",
      " 0.00699593 0.00642536 0.00639688 0.00589458 0.00542336 0.00525897\n",
      " 0.00489814 0.00469511 0.00458464 0.00443338 0.00401816 0.00390659\n",
      " 0.00367137 0.00351192 0.0032828  0.00320634 0.00292958 0.00283099\n",
      " 0.00259341 0.00258027 0.00245079 0.00225075 0.00212356 0.00206847\n",
      " 0.00197773 0.0018959  0.00177209 0.00170051 0.00160953 0.00141091\n",
      " 0.00137215 0.00129383 0.00112236 0.00106348 0.0010513  0.00103489\n",
      " 0.00085039 0.00081713 0.00076349 0.00073509 0.00068037 0.00065435\n",
      " 0.00061211 0.00060151 0.00056653 0.00050615]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA()\n",
    "pca.fit(X_scaled_train)\n",
    "X_pca_train = pca.transform(X_scaled_train)\n",
    "print(pca.explained_variance_)\n",
    "print(len(pca.explained_variance_))\n",
    "print(X_pca_train.shape)\n",
    "print(type(pca.explained_variance_))\n",
    "print(pca.explained_variance_.sum())\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "n_components_pca = 0\n",
    "\n",
    "percent_contrib = 0.05\n",
    "\n",
    "for i in range(len(pca.explained_variance_)):\n",
    "    if(pca.explained_variance_ratio_[i]>=(percent_contrib/100)):\n",
    "        n_components_pca+=1\n",
    "print(\"Number of PCA components that contribute >=\",percent_contrib,\"% of variance is \",n_components_pca)\n",
    "\n",
    "pca_red = PCA(n_components=n_components_pca)\n",
    "pca_red.fit(X_scaled_train)\n",
    "X_pca_red_train = pca_red.transform(X_scaled_train)\n",
    "X_pca_red_test = pca_red.transform(X_scaled_test)\n",
    "print(pca_red.explained_variance_)\n",
    "print(len(pca_red.explained_variance_))\n",
    "print(X_pca_red_train.shape)\n",
    "print(X_pca_red_test.shape)\n",
    "print(type(pca_red.explained_variance_))\n",
    "print(pca_red.explained_variance_.sum())\n",
    "print(pca_red.explained_variance_ratio_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399,)\n",
      "(399,)\n",
      "[0.0083686  0.25464095 0.23178747 0.03165719 0.23805097]\n",
      "[0.06 0.37 0.22 0.01 0.16]\n",
      "0.4650996056027381\n",
      "MAE  0.11773506953754113\n",
      "MSE  0.027374387903212383\n",
      "R2S  0.4650996056027381\n"
     ]
    }
   ],
   "source": [
    "clf_pca = linear_model.Lasso(alpha=0.1)\n",
    "clf_pca.fit(X_pca_red_train, y_train)\n",
    "y_pca_red_hat = clf_pca.predict(X_pca_red_test)\n",
    "print(y_pca_red_hat.shape)\n",
    "print(y_test.shape)\n",
    "print(y_pca_red_hat[0:5])\n",
    "print(y_test[0:5])\n",
    "print(clf_pca.score(X_pca_red_test,y_test))\n",
    "print(\"MAE \", mean_absolute_error(y_test, y_pca_red_hat))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE \", mean_squared_error(y_test, y_pca_red_hat))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2S \", r2_score(y_test, y_pca_red_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009796803105024275\n",
      "[0.03624396 0.21474003 0.25599261 0.02490614 0.23505143 0.07770956\n",
      " 0.23610201 0.17567949 0.36045802 0.32609378 0.41877535 0.03185056\n",
      " 0.16565426 0.26110014 0.05681181 0.10762168 0.11405087 0.3709334\n",
      " 0.03151875 0.32873991]\n",
      "[0.06 0.37 0.22 0.01 0.16 0.08 0.1 0.1 0.28 0.28 0.46 0.03 0.09 0.22 0.05\n",
      " 0.18 0.02 0.28 0.02 0.4]\n",
      "MAE  0.09302880299010646\n",
      "MSE  0.020225847304978336\n",
      "R2S  0.6047833566652235\n"
     ]
    }
   ],
   "source": [
    "lasso_pca_cv = LassoCV(alphas = None, cv = 10, max_iter = 1000, random_state=1) \n",
    "lasso_pca_cv.fit(X_pca_red_train, y_train)\n",
    "print(lasso_pca_cv.alpha_)\n",
    "\n",
    "y_pca_red_hat_cv = lasso_pca_cv.predict(X_pca_red_test)\n",
    "print(y_pca_red_hat_cv[0:20])\n",
    "print(y_test[0:20])\n",
    "print(\"MAE \", mean_absolute_error(y_test, y_pca_red_hat_cv))\n",
    "print(\"MSE \", mean_squared_error(y_test, y_pca_red_hat_cv))\n",
    "print(\"R2S \", r2_score(y_test, y_pca_red_hat_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
